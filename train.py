#ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ê¸°ì¡´ê³¼ ë™ì¼)
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, get_linear_schedule_with_warmup
import pandas as pd
import os
from model import AIHubEmotionDataset, EmotionClassifier, label_mapping
from tqdm import tqdm

# ğŸ”„ ì¦ê°•ìš© ì¶”ê°€ import
from deep_translator import GoogleTranslator
import time
import random
import torch.nn as nn
import torch.nn.functional as F

# Focal Loss í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        p_t = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        else:
            return focal_loss

#ğŸ—‚ï¸ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
project_path = '/content/drive/MyDrive/emotion_project'
continuous_dataset_file = f"{project_path}/í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹.xlsx"

# ğŸ¯ ëª©í‘œ ê°ì • ë¶„í¬
TARGET_DISTRIBUTION = {
    'ì¤‘ë¦½': 8000,    # 35%
    'ë¶„ë…¸': 3500,    # 15%
    'í–‰ë³µ': 3000,    # 13%
    'ìŠ¬í””': 2500,    # 11%
    'í˜ì˜¤': 2500,    # 11%
    'ë†€ëŒ': 2000,    # 9%
    'ê³µí¬': 1500,    # 6%
}

class ContextPreservedAugmentor:
    def __init__(self):
        self.languages = ['en', 'ja', 'zh']
    
    def augment_windowed_sentence(self, windowed_sentence, lang):
        """[SEP] í† í°ì´ í¬í•¨ëœ ë¬¸ì¥ì„ ë¬¸ë§¥ ë³´ì¡´í•˜ë©° ì¦ê°•"""
        try:
            # 1ë‹¨ê³„: [SEP] í† í° ì œê±°
            clean_text = windowed_sentence.replace(" [SEP] ", " ")
            
            # 2ë‹¨ê³„: ì „ì²´ ë¬¸ë§¥ ì—­ë²ˆì—­
            translator_to = GoogleTranslator(source='ko', target=lang)
            intermediate = translator_to.translate(clean_text)
            time.sleep(0.02)
            
            translator_back = GoogleTranslator(source=lang, target='ko')
            back_translated = translator_back.translate(intermediate)
            time.sleep(0.02)
            
            # 3ë‹¨ê³„: ì›ë³¸ [SEP] ê°œìˆ˜ í™•ì¸
            original_sep_count = windowed_sentence.count("[SEP]")
            
            if original_sep_count == 0:
                # ë‹¨ì¼ ë¬¸ì¥
                return back_translated
            else:
                # ë‹¤ì¤‘ ë¬¸ì¥: ê· ë“± ë¶„í•  í›„ [SEP] ì¬ì‚½ì…
                sentences = self.split_into_sentences(back_translated, original_sep_count + 1)
                return " [SEP] ".join(sentences)
                
        except Exception as e:
            print(f"ì¦ê°• ì‹¤íŒ¨: {e}")
            return windowed_sentence  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def split_into_sentences(self, text, target_count):
        """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê°œìˆ˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        words = text.split()
        if len(words) == 0:
            return [text] * target_count
        
        chunk_size = len(words) // target_count
        sentences = []
        
        for i in range(target_count):
            start = i * chunk_size
            if i == target_count - 1:  # ë§ˆì§€ë§‰ ì¡°ê°ì€ ë‚¨ì€ ëª¨ë“  ë‹¨ì–´
                end = len(words)
            else:
                end = start + chunk_size
            
            sentence = " ".join(words[start:end])
            sentences.append(sentence.strip())
        
        return sentences

# âœ¨ ê¸°ì¡´ load_data í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ìœ ì§€ (ìˆ˜ì • ì—†ìŒ)
def load_data(window_size=3):
    if not os.path.exists(continuous_dataset_file):
        raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: {continuous_dataset_file}")
    
    print(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ: {continuous_dataset_file}")
    df_raw = pd.read_excel(continuous_dataset_file, header=1)
    sentences = []
    emotions = []
    current_dialog = []
    
    for _, row in df_raw.iterrows():
        if pd.isna(row.iloc[1]):
            continue
            
        utterance = str(row.iloc[1]).strip()
        emotion = str(row.iloc[2]).strip() if pd.notna(row.iloc[2]) else "ì¤‘ë¦½"
        
        if pd.notna(row.iloc[0]) and str(row.iloc[0]).strip() == 'S':
            current_dialog = []
        
        current_dialog.append(utterance)
        window = current_dialog[-window_size:] if len(current_dialog) >= window_size else current_dialog
        sentence = " [SEP] ".join(window)
        
        sentences.append(sentence)
        emotions.append(emotion)
    
    def one_hot(emo):
        vec = [0.0] * 7
        idx = label_mapping.get(str(emo).strip())
        if idx is not None:
            vec[idx] = 1.0
        return vec
    
    emotion_vecs = [one_hot(emo) for emo in emotions]
    
    df = pd.DataFrame({
        'Sentence': sentences,
        'EmotionVec': emotion_vecs,
        'Emotion': emotions
    })
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ (ìœˆë„ìš° í¬ê¸°: {window_size})")
    
    emotion_dist = df['Emotion'].value_counts()
    print("\nğŸ“Š ê°ì • ë¶„í¬:")
    for emotion, count in emotion_dist.items():
        percentage = (count / len(df)) * 100
        print(f"  {emotion}: {count:,}ê°œ ({percentage:.1f}%)")
    
    return df

# âœ¨ ìƒˆë¡œ ì¶”ê°€: ì „ì²˜ë¦¬ëœ ë°ì´í„° ê¸°ë°˜ ì¦ê°• í•¨ìˆ˜
def augment_preprocessed_data(df, target_distribution):
    """ì´ë¯¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ê°€ ì ìš©ëœ DataFrameì„ ê¸°ë°˜ìœ¼ë¡œ ì¦ê°•"""
    
    print("\nğŸ”„ ì „ì²˜ë¦¬ëœ ë°ì´í„° ê¸°ë°˜ ë¬¸ë§¥ ë³´ì¡´ ì¦ê°• ì‹œì‘...")
    
    augmentor = ContextPreservedAugmentor()
    current_counts = df['Emotion'].value_counts()
    
    # ì¦ê°• ê³„íš ìˆ˜ë¦½
    augmentation_plan = {}
    for emotion, target in target_distribution.items():
        current = current_counts.get(emotion, 0)
        if current < target and current > 0:  # 0ê°œì¸ ê°ì •ì€ ì œì™¸
            needed_multiplier = min(int(target / current), 15)  # ìµœëŒ€ 15ë°° ì œí•œ
            augmentation_plan[emotion] = needed_multiplier
            print(f"  ğŸ“‹ {emotion}: {current}ê°œ â†’ ëª©í‘œ {target}ê°œ (Ã—{needed_multiplier} ì¦ê°•)")
    
    if not augmentation_plan:
        print("  â­ï¸ ì¦ê°•ì´ í•„ìš”í•œ ê°ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    # ê°ì •ë³„ ì¦ê°• ìˆ˜í–‰
    all_augmented_data = []
    
    for emotion, multiplier in augmentation_plan.items():
        print(f"\n  ğŸ¯ {emotion} ê°ì • ì¦ê°• ì¤‘... (Ã—{multiplier})")
        
        # í•´ë‹¹ ê°ì •ì˜ ëª¨ë“  ìƒ˜í”Œ ì¶”ì¶œ
        emotion_samples = df[df['Emotion'] == emotion].copy()
        print(f"    ì›ë³¸ {emotion} ìƒ˜í”Œ: {len(emotion_samples)}ê°œ")
        
        # ê° ì–¸ì–´ë³„ë¡œ ì¦ê°•
        for round_num in range(multiplier):
            lang = augmentor.languages[round_num % len(augmentor.languages)]
            print(f"    ë¼ìš´ë“œ {round_num+1}/{multiplier} - {lang.upper()} ì¦ê°• ì¤‘...")
            
            augmented_samples = []
            
            for idx, row in emotion_samples.iterrows():
                # ë¬¸ë§¥ ë³´ì¡´ ì¦ê°•
                augmented_sentence = augmentor.augment_windowed_sentence(
                    row['Sentence'], lang
                )
                
                augmented_samples.append({
                    'Sentence': augmented_sentence,
                    'EmotionVec': row['EmotionVec'],
                    'Emotion': row['Emotion']
                })
            
            round_df = pd.DataFrame(augmented_samples)
            all_augmented_data.append(round_df)
            
            print(f"      ì™„ë£Œ: {len(augmented_samples)}ê°œ ìƒ˜í”Œ ì¦ê°•")
    
    # ì›ë³¸ê³¼ ì¦ê°• ë°ì´í„° í•©ì¹˜ê¸°
    if all_augmented_data:
        augmented_df = pd.concat(all_augmented_data, ignore_index=True)
        combined_df = pd.concat([df, augmented_df], ignore_index=True)
        
        print(f"\nğŸ“Š ì¦ê°• í›„ ì´ ë°ì´í„°: {len(combined_df)}ê°œ")
        
        # ì¦ê°• í›„ ê°ì • ë¶„í¬
        final_counts = combined_df['Emotion'].value_counts()
        print("ì¦ê°• í›„ ê°ì • ë¶„í¬:")
        for emotion, count in final_counts.items():
            percentage = (count / len(combined_df)) * 100
            print(f"  {emotion}: {count:,}ê°œ ({percentage:.1f}%)")
        
        return combined_df
    else:
        return df

def balance_dataset(df, target_distribution):
    """ëª©í‘œ ë¶„í¬ì— ë§ì¶° ê· í˜• ì¡°ì •"""
    
    print("\nâš–ï¸ ê· í˜• ìƒ˜í”Œë§ ì ìš©...")
    
    balanced_parts = []
    current_counts = df['Emotion'].value_counts()
    
    for emotion, target_count in target_distribution.items():
        emotion_data = df[df['Emotion'] == emotion]
        current_count = len(emotion_data)
        
        if current_count >= target_count:
            sampled = emotion_data.sample(n=target_count, random_state=42)
            print(f"    {emotion}: {current_count:,} â†’ {target_count:,} (ìƒ˜í”Œë§)")
        else:
            sampled = emotion_data
            print(f"    {emotion}: {current_count:,} â†’ {current_count:,} (ì „ë¶€ ì‚¬ìš©)")
        
        balanced_parts.append(sampled)
    
    balanced_df = pd.concat(balanced_parts, ignore_index=True)
    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"ğŸ“Š ìµœì¢… ê· í˜• ë°ì´í„°: {len(balanced_df)}ê°œ")
    return balanced_df

# âœ¨ ìˆ˜ì •ëœ train í•¨ìˆ˜ (load_data í›„ ì¦ê°• ì¶”ê°€)
def train(start_epoch=0, total_epochs=10, window_size=3):
    pretrained_model_name = "klue/bert-base"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)

    # 1. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©ë¨)
    df = load_data(window_size)
    
    # 2. ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´ ì¦ê°•
    augmented_df = augment_preprocessed_data(df, TARGET_DISTRIBUTION)
    
    # 3. ê· í˜• ìƒ˜í”Œë§
    final_df = balance_dataset(augmented_df, TARGET_DISTRIBUTION)
    
    print("\nğŸ“Š ë°ì´í„° ë¶„í•  (í•™ìŠµ 80% : í‰ê°€ 20%)")
    train_df = final_df.sample(frac=0.8, random_state=42)
    eval_df = final_df.drop(train_df.index)
    
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_df):,}ê°œ ({len(train_df)/len(final_df)*100:.1f}%)")
    print(f"âœ… í‰ê°€ ë°ì´í„°: {len(eval_df):,}ê°œ ({len(eval_df)/len(final_df)*100:.1f}%)")
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    train_dataset = AIHubEmotionDataset(train_df, tokenizer)
    eval_dataset = AIHubEmotionDataset(eval_df, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    eval_loader = DataLoader(eval_dataset, batch_size=32)

    # ëª¨ë¸ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    model = EmotionClassifier(pretrained_model_name, num_classes=7).to(device)

    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=3e-5)
    loss_fn = FocalLoss(alpha=1.0, gamma=2.0)  # Focal Loss ì‚¬ìš©
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=len(train_loader) * 2,
        num_training_steps=len(train_loader)*total_epochs
    )

    print(f"\nğŸš€ Focal Lossë¡œ í•™ìŠµ ì‹œì‘ (ì´ {total_epochs}ê°œ ì—í¬í¬)")
    
    best_eval_loss = float('inf')
    
    # í•™ìŠµ ë£¨í”„ (ê¸°ì¡´ê³¼ ì™„ì „ ë™ì¼)
    for epoch in range(start_epoch, total_epochs):
        model.train()
        total_train_loss = 0
        
        train_loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{total_epochs} - Training")
        for batch in train_loop:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_train_loss += loss.item()
            train_loop.set_postfix(loss=loss.item())

        avg_train_loss = total_train_loss / len(train_loader)
        
        model.eval()
        total_eval_loss = 0
        
        eval_loop = tqdm(eval_loader, desc=f"Epoch {epoch+1}/{total_epochs} - Evaluation")
        with torch.no_grad():
            for batch in eval_loop:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = loss_fn(outputs, labels)
                
                total_eval_loss += loss.item()
                eval_loop.set_postfix(loss=loss.item())
        
        avg_eval_loss = total_eval_loss / len(eval_loader)
        
        print(f"âœ… Epoch {epoch+1}")
        print(f"   ğŸ“š Train Loss: {avg_train_loss:.4f}")
        print(f"   ğŸ“Š Eval Loss:  {avg_eval_loss:.4f}")
        
        if avg_eval_loss < best_eval_loss:
            best_eval_loss = avg_eval_loss
            best_model_path = f"{project_path}/best_model.pth"
            torch.save(model.state_dict(), best_model_path)
            print(f"   ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! (Eval Loss: {avg_eval_loss:.4f})")

    final_save_path = f"{project_path}/model.pth"
    torch.save(model.state_dict(), final_save_path)
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ“ ìµœì¢… ëª¨ë¸: {final_save_path}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_path}")

# ì‹¤í–‰
if __name__ == "__main__":
    train(total_epochs=8)